{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b08640",
   "metadata": {},
   "source": [
    "# CORD-19 Research Dataset Analysis\n",
    "\n",
    "## Assignment Overview\n",
    "This notebook provides a comprehensive analysis of the CORD-19 research dataset, focusing on COVID-19 research papers metadata. We'll explore the data, clean it, create visualizations, and prepare components for a Streamlit application.\n",
    "\n",
    "### Learning Objectives:\n",
    "- Practice loading and exploring real-world datasets\n",
    "- Learn basic data cleaning techniques\n",
    "- Create meaningful visualizations\n",
    "- Build components for interactive web applications\n",
    "- Present data insights effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a862fdc",
   "metadata": {},
   "source": [
    "## Part 1: Import Required Libraries\n",
    "Import all necessary libraries for data analysis, visualization, and text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26718168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text processing and word clouds\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime, date\n",
    "import warnings\n",
    "\n",
    "# For downloading data\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb57377",
   "metadata": {},
   "source": [
    "## Part 2: Download and Load the CORD-19 Dataset\n",
    "\n",
    "**Note**: For this example, we'll create a sample dataset since the full CORD-19 dataset is very large. In a real scenario, you would download the metadata.csv file from Kaggle.\n",
    "\n",
    "### Instructions for downloading the real dataset:\n",
    "1. Go to https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n",
    "2. Download only the `metadata.csv` file\n",
    "3. Place it in the `data/` directory\n",
    "4. Uncomment the real data loading code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce50a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll create a sample dataset\n",
    "# In practice, you would load the real CORD-19 metadata\n",
    "\n",
    "def create_sample_cord19_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Create a sample dataset that mimics the CORD-19 metadata structure\n",
    "    This is for demonstration purposes only.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample journals\n",
    "    journals = [\n",
    "        'Nature', 'Science', 'The Lancet', 'New England Journal of Medicine',\n",
    "        'Cell', 'PLOS ONE', 'Nature Medicine', 'Journal of Virology',\n",
    "        'Virology', 'Nature Communications', 'BMJ', 'JAMA',\n",
    "        'Proceedings of the National Academy of Sciences', 'Nature Microbiology',\n",
    "        'Clinical Infectious Diseases', 'Emerging Infectious Diseases'\n",
    "    ]\n",
    "    \n",
    "    # Sample COVID-related terms for titles\n",
    "    covid_terms = [\n",
    "        'COVID-19', 'SARS-CoV-2', 'coronavirus', 'pandemic', 'vaccine',\n",
    "        'antiviral', 'treatment', 'symptoms', 'transmission', 'respiratory',\n",
    "        'infection', 'immunity', 'antibody', 'outbreak', 'diagnosis',\n",
    "        'therapeutic', 'prevention', 'epidemiology', 'public health', 'clinical'\n",
    "    ]\n",
    "    \n",
    "    # Generate sample data\n",
    "    data = {\n",
    "        'cord_uid': [f'cord-{i:06d}' for i in range(n_samples)],\n",
    "        'title': [f'{np.random.choice(covid_terms)} {np.random.choice([\"study\", \"analysis\", \"research\", \"investigation\", \"treatment\", \"vaccine\", \"therapy\"])} in {np.random.choice([\"patients\", \"population\", \"healthcare workers\", \"elderly\", \"children\"])}: {np.random.choice([\"a systematic review\", \"clinical trial\", \"observational study\", \"meta-analysis\", \"case series\"])}' for _ in range(n_samples)],\n",
    "        'abstract': [f'This study investigates {np.random.choice(covid_terms).lower()} in the context of pandemic response. Methods included analysis of {np.random.randint(50, 5000)} participants over {np.random.randint(1, 24)} months.' for _ in range(n_samples)],\n",
    "        'authors': [f'Author{i % 100}, J.; Smith, A.; Johnson, B.' for i in range(n_samples)],\n",
    "        'journal': np.random.choice(journals, n_samples),\n",
    "        'publish_time': pd.date_range(start='2019-12-01', end='2023-12-31', periods=n_samples),\n",
    "        'source_x': np.random.choice(['PMC', 'Elsevier', 'arXiv', 'bioRxiv', 'medRxiv'], n_samples),\n",
    "        'pmcid': [f'PMC{np.random.randint(1000000, 9999999)}' if np.random.random() > 0.3 else None for _ in range(n_samples)],\n",
    "        'pubmed_id': [np.random.randint(10000000, 99999999) if np.random.random() > 0.2 else None for _ in range(n_samples)],\n",
    "        'license': np.random.choice(['cc-by', 'cc-by-nc', 'cc-by-sa', 'els-covid', 'arxiv'], n_samples),\n",
    "        'has_full_text': np.random.choice([True, False], n_samples, p=[0.7, 0.3])\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create sample data\n",
    "print(\"Creating sample CORD-19 dataset...\")\n",
    "df = create_sample_cord19_data(2000)\n",
    "\n",
    "# Uncomment below to load real CORD-19 data\n",
    "# df = pd.read_csv('../data/metadata.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb84721",
   "metadata": {},
   "source": [
    "## Part 3: Basic Data Exploration\n",
    "Let's examine the structure and basic properties of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(\"\\n=== COLUMN NAMES ===\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"=== FIRST 5 ROWS ===\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69263556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "print(\"=== DATA TYPES AND MEMORY USAGE ===\")\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b11dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for numerical columns\n",
    "print(\"=== BASIC STATISTICS ===\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b287b74",
   "metadata": {},
   "source": [
    "## Part 4: Data Cleaning and Missing Value Handling\n",
    "Identify and handle missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b274c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd200865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_counts) > 0:\n",
    "    plt.bar(range(len(missing_counts)), missing_counts.values)\n",
    "    plt.xticks(range(len(missing_counts)), missing_counts.index, rotation=45)\n",
    "    plt.title('Missing Values by Column')\n",
    "    plt.ylabel('Count of Missing Values')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned version of the dataset\n",
    "print(\"=== DATA CLEANING ===\")\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove rows where title is missing (critical field)\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.dropna(subset=['title'])\n",
    "print(f\"Removed {initial_rows - len(df_clean)} rows with missing titles\")\n",
    "\n",
    "# Fill missing abstracts with placeholder\n",
    "df_clean['abstract'] = df_clean['abstract'].fillna('Abstract not available')\n",
    "\n",
    "# Fill missing journal names\n",
    "df_clean['journal'] = df_clean['journal'].fillna('Unknown Journal')\n",
    "\n",
    "# For other missing values, we'll keep them as NaN for now\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Rows retained: {len(df_clean)/initial_rows*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad2d2e",
   "metadata": {},
   "source": [
    "## Part 5: Data Type Conversion and Feature Engineering\n",
    "Convert data types and create new features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publish_time to datetime if it's not already\n",
    "print(\"=== DATA TYPE CONVERSION ===\")\n",
    "if df_clean['publish_time'].dtype == 'object':\n",
    "    df_clean['publish_time'] = pd.to_datetime(df_clean['publish_time'], errors='coerce')\n",
    "    print(\"Converted publish_time to datetime\")\n",
    "else:\n",
    "    print(\"publish_time is already datetime format\")\n",
    "\n",
    "# Extract year from publication date\n",
    "df_clean['publication_year'] = df_clean['publish_time'].dt.year\n",
    "print(\"Created publication_year feature\")\n",
    "\n",
    "# Extract month for seasonal analysis\n",
    "df_clean['publication_month'] = df_clean['publish_time'].dt.month\n",
    "df_clean['publication_month_name'] = df_clean['publish_time'].dt.month_name()\n",
    "print(\"Created publication_month features\")\n",
    "\n",
    "# Create abstract word count feature\n",
    "df_clean['abstract_word_count'] = df_clean['abstract'].str.split().str.len()\n",
    "print(\"Created abstract_word_count feature\")\n",
    "\n",
    "# Create title word count feature\n",
    "df_clean['title_word_count'] = df_clean['title'].str.split().str.len()\n",
    "print(\"Created title_word_count feature\")\n",
    "\n",
    "# Create title length feature\n",
    "df_clean['title_length'] = df_clean['title'].str.len()\n",
    "print(\"Created title_length feature\")\n",
    "\n",
    "print(f\"\\nDataset now has {df_clean.shape[1]} columns (added {df_clean.shape[1] - df.shape[1]} new features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92771c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the new features\n",
    "print(\"=== NEW FEATURES SUMMARY ===\")\n",
    "new_features = ['publication_year', 'publication_month', 'publication_month_name', \n",
    "                'abstract_word_count', 'title_word_count', 'title_length']\n",
    "\n",
    "for feature in new_features:\n",
    "    if feature in df_clean.columns:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        if df_clean[feature].dtype in ['int64', 'float64']:\n",
    "            print(f\"  Range: {df_clean[feature].min()} - {df_clean[feature].max()}\")\n",
    "            print(f\"  Mean: {df_clean[feature].mean():.1f}\")\n",
    "        else:\n",
    "            print(f\"  Unique values: {df_clean[feature].nunique()}\")\n",
    "            print(f\"  Sample values: {list(df_clean[feature].value_counts().head(3).index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213be230",
   "metadata": {},
   "source": [
    "## Part 6: Publication Year Analysis\n",
    "Analyze the distribution of papers by publication year to understand research trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze publications by year\n",
    "print(\"=== PUBLICATION YEAR ANALYSIS ===\")\n",
    "year_counts = df_clean['publication_year'].value_counts().sort_index()\n",
    "print(\"Papers by year:\")\n",
    "for year, count in year_counts.items():\n",
    "    if pd.notna(year):\n",
    "        print(f\"  {int(year)}: {count:,} papers\")\n",
    "\n",
    "print(f\"\\nTotal papers with valid publication year: {year_counts.sum():,}\")\n",
    "print(f\"Peak year: {year_counts.idxmax()} with {year_counts.max():,} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1907b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create publication year distribution visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "year_counts_clean = year_counts.dropna()\n",
    "plt.bar(year_counts_clean.index, year_counts_clean.values, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "plt.title('Distribution of COVID-19 Research Papers by Publication Year', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Publication Year', fontsize=12)\n",
    "plt.ylabel('Number of Papers', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for year, count in year_counts_clean.items():\n",
    "    plt.text(year, count + max(year_counts_clean) * 0.01, str(count), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28e33f",
   "metadata": {},
   "source": [
    "## Part 7: Journal and Source Analysis\n",
    "Identify the top journals and sources publishing COVID-19 research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top journals\n",
    "print(\"=== TOP JOURNALS ANALYSIS ===\")\n",
    "top_journals = df_clean['journal'].value_counts().head(15)\n",
    "print(\"Top 15 journals by publication count:\")\n",
    "for i, (journal, count) in enumerate(top_journals.items(), 1):\n",
    "    print(f\"{i:2d}. {journal}: {count:,} papers\")\n",
    "\n",
    "print(f\"\\nTotal unique journals: {df_clean['journal'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sources\n",
    "print(\"\\n=== SOURCE ANALYSIS ===\")\n",
    "source_counts = df_clean['source_x'].value_counts()\n",
    "print(\"Papers by source:\")\n",
    "for source, count in source_counts.items():\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"  {source}: {count:,} papers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top journals\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Top journals bar chart\n",
    "top_10_journals = df_clean['journal'].value_counts().head(10)\n",
    "ax1.barh(range(len(top_10_journals)), top_10_journals.values, color='lightcoral')\n",
    "ax1.set_yticks(range(len(top_10_journals)))\n",
    "ax1.set_yticklabels([j[:50] + '...' if len(j) > 50 else j for j in top_10_journals.index])\n",
    "ax1.set_xlabel('Number of Papers')\n",
    "ax1.set_title('Top 10 Journals Publishing COVID-19 Research', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(top_10_journals.values):\n",
    "    ax1.text(v + max(top_10_journals.values) * 0.01, i, str(v), \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "# Source distribution pie chart\n",
    "ax2.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%', \n",
    "        colors=plt.cm.Set3.colors, startangle=90)\n",
    "ax2.set_title('Distribution of Papers by Source', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c997e",
   "metadata": {},
   "source": [
    "## Part 8: Title Text Analysis and Word Frequency\n",
    "Analyze the most common words and terms in paper titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7451e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze words from titles\n",
    "print(\"=== TITLE TEXT ANALYSIS ===\")\n",
    "\n",
    "def extract_words(text_series, min_length=3):\n",
    "    \"\"\"\n",
    "    Extract words from text, removing common stop words and short words\n",
    "    \"\"\"\n",
    "    # Common stop words to remove\n",
    "    stop_words = {\n",
    "        'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "        'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after',\n",
    "        'above', 'below', 'between', 'among', 'throughout', 'despite', 'towards',\n",
    "        'upon', 'concerning', 'a', 'an', 'as', 'are', 'was', 'were', 'been', 'be',\n",
    "        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',\n",
    "        'may', 'might', 'must', 'can', 'shall', 'is', 'this', 'that', 'these', 'those'\n",
    "    }\n",
    "    \n",
    "    all_words = []\n",
    "    for text in text_series.dropna():\n",
    "        # Convert to lowercase and extract words\n",
    "        words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "        # Filter words\n",
    "        filtered_words = [w for w in words if len(w) >= min_length and w not in stop_words]\n",
    "        all_words.extend(filtered_words)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "# Extract words from titles\n",
    "title_words = extract_words(df_clean['title'])\n",
    "word_freq = Counter(title_words)\n",
    "\n",
    "print(f\"Total words extracted: {len(title_words):,}\")\n",
    "print(f\"Unique words: {len(word_freq):,}\")\n",
    "print(\"\\nTop 20 most frequent words in titles:\")\n",
    "for i, (word, count) in enumerate(word_freq.most_common(20), 1):\n",
    "    print(f\"{i:2d}. {word}: {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90626c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word frequency\n",
    "top_words = word_freq.most_common(15)\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(range(len(words)), counts, color='mediumseagreen')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Most Frequent Words in COVID-19 Research Paper Titles', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()  # Show highest frequency at top\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "    plt.text(count + max(counts) * 0.01, i, str(count), \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105489f",
   "metadata": {},
   "source": [
    "## Part 9: Time Series Visualization of Publications\n",
    "Create detailed time series plots showing publication trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monthly publication timeline\n",
    "df_clean['year_month'] = df_clean['publish_time'].dt.to_period('M')\n",
    "monthly_counts = df_clean['year_month'].value_counts().sort_index()\n",
    "\n",
    "# Convert Period index to datetime for plotting\n",
    "monthly_counts.index = monthly_counts.index.to_timestamp()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(monthly_counts.index, monthly_counts.values, linewidth=2, color='darkblue', marker='o', markersize=4)\n",
    "plt.title('COVID-19 Research Publications Over Time (Monthly)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Number of Papers Published', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Highlight peak months\n",
    "peak_month = monthly_counts.idxmax()\n",
    "peak_count = monthly_counts.max()\n",
    "plt.annotate(f'Peak: {peak_count} papers\\n{peak_month.strftime(\"%B %Y\")}', \n",
    "             xy=(peak_month, peak_count), xytext=(10, 10),\n",
    "             textcoords='offset points', bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Peak publication month: {peak_month.strftime('%B %Y')} with {peak_count} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4299c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cumulative publications plot\n",
    "cumulative_counts = monthly_counts.cumsum()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Monthly publications\n",
    "ax1.plot(monthly_counts.index, monthly_counts.values, linewidth=2, color='darkred', marker='o', markersize=3)\n",
    "ax1.set_title('Monthly COVID-19 Research Publications', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Papers per Month')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cumulative publications\n",
    "ax2.plot(cumulative_counts.index, cumulative_counts.values, linewidth=3, color='darkgreen')\n",
    "ax2.fill_between(cumulative_counts.index, cumulative_counts.values, alpha=0.3, color='lightgreen')\n",
    "ax2.set_title('Cumulative COVID-19 Research Publications', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Total Papers Published')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total papers in dataset: {cumulative_counts.iloc[-1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68eba48",
   "metadata": {},
   "source": [
    "## Part 10: Journal Publication Distribution Charts\n",
    "Create detailed visualizations of journal publication patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze journal publication patterns over time\n",
    "top_5_journals = df_clean['journal'].value_counts().head(5).index\n",
    "journal_year_data = df_clean[df_clean['journal'].isin(top_5_journals)].groupby(['publication_year', 'journal']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create stacked area chart\n",
    "plt.figure(figsize=(14, 8))\n",
    "journal_year_data.plot(kind='area', stacked=True, alpha=0.7, figsize=(14, 8))\n",
    "plt.title('Publication Trends by Top 5 Journals Over Time', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Publication Year', fontsize=12)\n",
    "plt.ylabel('Number of Papers', fontsize=12)\n",
    "plt.legend(title='Journal', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create journal impact visualization (papers vs average title length)\n",
    "journal_stats = df_clean.groupby('journal').agg({\n",
    "    'cord_uid': 'count',\n",
    "    'title_length': 'mean',\n",
    "    'abstract_word_count': 'mean'\n",
    "}).rename(columns={'cord_uid': 'paper_count'})\n",
    "\n",
    "# Filter to journals with at least 10 papers\n",
    "journal_stats_filtered = journal_stats[journal_stats['paper_count'] >= 10].head(20)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# 1. Paper count vs average title length\n",
    "scatter = ax1.scatter(journal_stats_filtered['paper_count'], \n",
    "                     journal_stats_filtered['title_length'],\n",
    "                     alpha=0.6, s=100, c='purple')\n",
    "ax1.set_xlabel('Number of Papers')\n",
    "ax1.set_ylabel('Average Title Length (characters)')\n",
    "ax1.set_title('Journal Paper Count vs Average Title Length')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Top journals by paper count\n",
    "top_journals_count = journal_stats_filtered.nlargest(10, 'paper_count')\n",
    "ax2.barh(range(len(top_journals_count)), top_journals_count['paper_count'], color='orange')\n",
    "ax2.set_yticks(range(len(top_journals_count)))\n",
    "ax2.set_yticklabels([j[:30] + '...' if len(j) > 30 else j for j in top_journals_count.index])\n",
    "ax2.set_xlabel('Number of Papers')\n",
    "ax2.set_title('Top 10 Journals by Paper Count')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Distribution of papers per journal\n",
    "all_journal_counts = df_clean['journal'].value_counts()\n",
    "ax3.hist(all_journal_counts.values, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax3.set_xlabel('Papers per Journal')\n",
    "ax3.set_ylabel('Number of Journals')\n",
    "ax3.set_title('Distribution of Papers per Journal')\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Journal diversity over time\n",
    "yearly_journal_diversity = df_clean.groupby('publication_year')['journal'].nunique()\n",
    "ax4.plot(yearly_journal_diversity.index, yearly_journal_diversity.values, \n",
    "         marker='o', linewidth=2, color='red')\n",
    "ax4.set_xlabel('Publication Year')\n",
    "ax4.set_ylabel('Number of Unique Journals')\n",
    "ax4.set_title('Journal Diversity by Year')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total unique journals: {df_clean['journal'].nunique():,}\")\n",
    "print(f\"Journals with only one paper: {sum(all_journal_counts == 1):,} ({sum(all_journal_counts == 1)/len(all_journal_counts)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58098426",
   "metadata": {},
   "source": [
    "## Part 11: Word Cloud Generation from Titles\n",
    "Create visual word clouds to represent the most common terms in research titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud from titles\n",
    "print(\"=== GENERATING WORD CLOUD ===\")\n",
    "\n",
    "# Combine all titles into one text\n",
    "all_titles_text = ' '.join(df_clean['title'].dropna().astype(str))\n",
    "\n",
    "# Remove common words and create word cloud\n",
    "additional_stopwords = {\n",
    "    'study', 'analysis', 'research', 'using', 'based', 'case', 'patients', 'patient',\n",
    "    'clinical', 'systematic', 'review', 'meta', 'observational', 'trial', 'results',\n",
    "    'among', 'associated', 'association', 'factors', 'risk', 'retrospective'\n",
    "}\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=1200, \n",
    "    height=600, \n",
    "    background_color='white',\n",
    "    max_words=100,\n",
    "    stopwords=additional_stopwords,\n",
    "    colormap='viridis',\n",
    "    relative_scaling=0.5,\n",
    "    min_font_size=10\n",
    ").generate(all_titles_text)\n",
    "\n",
    "# Display word cloud\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of COVID-19 Research Paper Titles', fontsize=20, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Word cloud generated from {len(df_clean['title'].dropna()):,} titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate word clouds for different years\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get top 4 years by paper count\n",
    "top_years = df_clean['publication_year'].value_counts().head(4).index\n",
    "\n",
    "for i, year in enumerate(top_years):\n",
    "    if pd.isna(year):\n",
    "        continue\n",
    "        \n",
    "    year_titles = df_clean[df_clean['publication_year'] == year]['title'].dropna()\n",
    "    year_text = ' '.join(year_titles.astype(str))\n",
    "    \n",
    "    if len(year_text.strip()) > 0:\n",
    "        year_wordcloud = WordCloud(\n",
    "            width=600, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            max_words=50,\n",
    "            stopwords=additional_stopwords,\n",
    "            colormap='Set2',\n",
    "            relative_scaling=0.5\n",
    "        ).generate(year_text)\n",
    "        \n",
    "        axes[i].imshow(year_wordcloud, interpolation='bilinear')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f'{int(year)} ({len(year_titles)} papers)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Word Clouds by Publication Year', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dbcf5",
   "metadata": {},
   "source": [
    "## Part 12: Source Distribution Analysis\n",
    "Analyze the distribution of papers across different sources and repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive source analysis\n",
    "print(\"=== COMPREHENSIVE SOURCE ANALYSIS ===\")\n",
    "\n",
    "# Source distribution\n",
    "source_stats = df_clean.groupby('source_x').agg({\n",
    "    'cord_uid': 'count',\n",
    "    'has_full_text': lambda x: sum(x == True),\n",
    "    'title_length': 'mean',\n",
    "    'abstract_word_count': 'mean'\n",
    "}).rename(columns={\n",
    "    'cord_uid': 'paper_count',\n",
    "    'has_full_text': 'full_text_count'\n",
    "})\n",
    "\n",
    "source_stats['full_text_percentage'] = (source_stats['full_text_count'] / source_stats['paper_count']) * 100\n",
    "\n",
    "print(\"Source statistics:\")\n",
    "for source in source_stats.index:\n",
    "    stats = source_stats.loc[source]\n",
    "    print(f\"\\n{source}:\")\n",
    "    print(f\"  Papers: {stats['paper_count']:,}\")\n",
    "    print(f\"  Full text available: {stats['full_text_count']:,} ({stats['full_text_percentage']:.1f}%)\")\n",
    "    print(f\"  Avg title length: {stats['title_length']:.1f} characters\")\n",
    "    print(f\"  Avg abstract words: {stats['abstract_word_count']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd78ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive source visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Source distribution pie chart\n",
    "colors = plt.cm.Set3.colors\n",
    "wedges, texts, autotexts = ax1.pie(source_stats['paper_count'], \n",
    "                                  labels=source_stats.index, \n",
    "                                  autopct='%1.1f%%',\n",
    "                                  colors=colors,\n",
    "                                  startangle=90)\n",
    "ax1.set_title('Distribution of Papers by Source', fontweight='bold')\n",
    "\n",
    "# 2. Full text availability by source\n",
    "ax2.bar(source_stats.index, source_stats['full_text_percentage'], color='lightblue', edgecolor='navy')\n",
    "ax2.set_title('Full Text Availability by Source', fontweight='bold')\n",
    "ax2.set_ylabel('Percentage with Full Text')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(source_stats['full_text_percentage']):\n",
    "    ax2.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Source publication trends over time\n",
    "source_year = df_clean.groupby(['publication_year', 'source_x']).size().unstack(fill_value=0)\n",
    "source_year.plot(kind='line', ax=ax3, marker='o', linewidth=2)\n",
    "ax3.set_title('Publication Trends by Source Over Time', fontweight='bold')\n",
    "ax3.set_xlabel('Publication Year')\n",
    "ax3.set_ylabel('Number of Papers')\n",
    "ax3.legend(title='Source', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Average metrics by source\n",
    "x_pos = np.arange(len(source_stats.index))\n",
    "width = 0.35\n",
    "\n",
    "# Normalize metrics for comparison\n",
    "norm_title_length = source_stats['title_length'] / source_stats['title_length'].max() * 100\n",
    "norm_abstract_words = source_stats['abstract_word_count'] / source_stats['abstract_word_count'].max() * 100\n",
    "\n",
    "ax4.bar(x_pos - width/2, norm_title_length, width, label='Title Length (normalized)', alpha=0.7)\n",
    "ax4.bar(x_pos + width/2, norm_abstract_words, width, label='Abstract Words (normalized)', alpha=0.7)\n",
    "ax4.set_title('Average Metrics by Source (Normalized)', fontweight='bold')\n",
    "ax4.set_ylabel('Normalized Score (0-100)')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(source_stats.index, rotation=45)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d283e",
   "metadata": {},
   "source": [
    "## Part 13: Create Streamlit Application Structure\n",
    "Build the foundation for our interactive Streamlit web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data for Streamlit app\n",
    "print(\"=== PREPARING DATA FOR STREAMLIT APP ===\")\n",
    "\n",
    "# Create a sample of the data for the app (to keep it responsive)\n",
    "sample_size = min(1000, len(df_clean))\n",
    "df_sample = df_clean.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Save the sample data\n",
    "df_sample.to_csv('../data/cord19_sample.csv', index=False)\n",
    "print(f\"Saved sample dataset with {len(df_sample):,} records to '../data/cord19_sample.csv'\")\n",
    "\n",
    "# Also save key statistics for the app\n",
    "app_stats = {\n",
    "    'total_papers': len(df_clean),\n",
    "    'date_range': f\"{df_clean['publish_time'].min().strftime('%Y-%m-%d')} to {df_clean['publish_time'].max().strftime('%Y-%m-%d')}\",\n",
    "    'unique_journals': df_clean['journal'].nunique(),\n",
    "    'top_journal': df_clean['journal'].value_counts().index[0],\n",
    "    'peak_year': df_clean['publication_year'].value_counts().index[0],\n",
    "    'sources': list(df_clean['source_x'].unique())\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/app_stats.json', 'w') as f:\n",
    "    json.dump(app_stats, f, indent=2, default=str)\n",
    "\n",
    "print(\"Saved app statistics to '../data/app_stats.json'\")\n",
    "print(\"\\nKey statistics for the app:\")\n",
    "for key, value in app_stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a53a3b",
   "metadata": {},
   "source": [
    "## Part 14: Summary and Key Findings\n",
    "Summarize our analysis and key insights from the CORD-19 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af085396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=== CORD-19 DATASET ANALYSIS SUMMARY ===\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS AND INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"\\n📊 DATASET OVERVIEW:\")\n",
    "print(f\"   • Total papers analyzed: {len(df_clean):,}\")\n",
    "print(f\"   • Date range: {df_clean['publish_time'].min().strftime('%B %Y')} - {df_clean['publish_time'].max().strftime('%B %Y')}\")\n",
    "print(f\"   • Unique journals: {df_clean['journal'].nunique():,}\")\n",
    "print(f\"   • Data sources: {', '.join(df_clean['source_x'].unique())}\")\n",
    "\n",
    "# Publication trends\n",
    "print(f\"\\n📈 PUBLICATION TRENDS:\")\n",
    "peak_year = df_clean['publication_year'].value_counts().index[0]\n",
    "peak_count = df_clean['publication_year'].value_counts().iloc[0]\n",
    "print(f\"   • Peak publication year: {int(peak_year)} ({peak_count:,} papers)\")\n",
    "\n",
    "monthly_peak = df_clean['year_month'].value_counts().index[0]\n",
    "monthly_peak_count = df_clean['year_month'].value_counts().iloc[0]\n",
    "print(f\"   • Peak publication month: {monthly_peak} ({monthly_peak_count:,} papers)\")\n",
    "\n",
    "# Journal insights\n",
    "print(f\"\\n📚 JOURNAL INSIGHTS:\")\n",
    "top_journal = df_clean['journal'].value_counts().index[0]\n",
    "top_journal_count = df_clean['journal'].value_counts().iloc[0]\n",
    "print(f\"   • Top publishing journal: {top_journal} ({top_journal_count:,} papers)\")\n",
    "\n",
    "single_paper_journals = sum(df_clean['journal'].value_counts() == 1)\n",
    "print(f\"   • Journals with only one paper: {single_paper_journals:,} ({single_paper_journals/df_clean['journal'].nunique()*100:.1f}% of all journals)\")\n",
    "\n",
    "# Content analysis\n",
    "print(f\"\\n📝 CONTENT ANALYSIS:\")\n",
    "avg_title_length = df_clean['title_length'].mean()\n",
    "avg_abstract_words = df_clean['abstract_word_count'].mean()\n",
    "print(f\"   • Average title length: {avg_title_length:.1f} characters\")\n",
    "print(f\"   • Average abstract word count: {avg_abstract_words:.1f} words\")\n",
    "\n",
    "top_words = [word for word, _ in word_freq.most_common(5)]\n",
    "print(f\"   • Most frequent title terms: {', '.join(top_words)}\")\n",
    "\n",
    "# Source analysis\n",
    "print(f\"\\n🗄️ SOURCE ANALYSIS:\")\n",
    "for source in df_clean['source_x'].value_counts().index:\n",
    "    count = df_clean['source_x'].value_counts()[source]\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    full_text_pct = (df_clean[df_clean['source_x'] == source]['has_full_text'].sum() / count) * 100\n",
    "    print(f\"   • {source}: {count:,} papers ({percentage:.1f}%), {full_text_pct:.1f}% with full text\")\n",
    "\n",
    "# Research focus areas (based on title analysis)\n",
    "print(f\"\\n🔬 RESEARCH FOCUS AREAS (based on title analysis):\")\n",
    "covid_keywords = ['covid', 'coronavirus', 'sars', 'pandemic', 'vaccine', 'treatment', 'clinical', 'patients']\n",
    "for keyword in covid_keywords:\n",
    "    if keyword in word_freq:\n",
    "        count = word_freq[keyword]\n",
    "        percentage = (count / len(title_words)) * 100\n",
    "        print(f\"   • {keyword.capitalize()}: {count:,} mentions ({percentage:.1f}% of all title words)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE - DATA READY FOR STREAMLIT APP\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fccd42",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This analysis provides a comprehensive foundation for understanding the CORD-19 research dataset. The key components have been prepared for integration into a Streamlit application:\n",
    "\n",
    "### What we've accomplished:\n",
    "1. ✅ **Data Loading & Exploration** - Loaded and examined the dataset structure\n",
    "2. ✅ **Data Cleaning** - Handled missing values and prepared clean data\n",
    "3. ✅ **Feature Engineering** - Created new features for analysis\n",
    "4. ✅ **Publication Analysis** - Analyzed trends over time and by year\n",
    "5. ✅ **Journal Analysis** - Identified top publishers and publication patterns\n",
    "6. ✅ **Text Analysis** - Extracted insights from titles and abstracts\n",
    "7. ✅ **Visualizations** - Created comprehensive charts and graphs\n",
    "8. ✅ **Word Clouds** - Generated visual representations of key terms\n",
    "9. ✅ **Source Analysis** - Analyzed distribution across data sources\n",
    "10. ✅ **Data Export** - Prepared datasets for Streamlit application\n",
    "\n",
    "### Ready for Streamlit App Development:\n",
    "- Sample dataset saved: `../data/cord19_sample.csv`\n",
    "- Statistics saved: `../data/app_stats.json`\n",
    "- All visualizations and analysis functions ready for integration\n",
    "\n",
    "The next step is to create the Streamlit application that will make this analysis interactive and accessible to users."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
